# -*- coding: utf-8 -*-
"""ApexAssessment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1esRhSy4HppZd2KnNc9sBZ_jQ7vHAFTku

Python Version- 3.7
Pandas Version- 1.0.3
"""

# Commented out IPython magic to ensure Python compatibility.
#Import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import datetime
import pandas_profiling
# %matplotlib inline

"""##Append the data sets together from the two the sheets."""

#The excel sheet can be downloaded to the local machine 
sheets = pd.read_excel('/content/online_retail_II (1).xlsx', sheet_name=None)

#The sheet can't be read from the given url, due to SSL certification issue on my machine, faced an issue executing this command
#s = pd.read_excel('https://archive.ics.uci.edu/ml/machine-learning-databases/00502/online_retail_II.xlsx', sheet_name= None)

#merge the sheets and read it into a dataframe
df = pd.concat(sheets[frame] for frame in sheets.keys())

#Print the total number of records of the dataframe
print("Total no of rows:",len(df))

"""##Profiling of Data- Statistics"""

#To preview the data
df.head()

#To get the information of the columns of the dataframe
df.info()

#Describe the dataframe to get statistics- mean, min, count, max
df.describe().round(2)

#Check the dataframe for null values
df_missing=df.isnull().sum().sort_values()
df_missing

#Plot the missing values for each column of the dataframe
df_missing.plot(kind='bar',color='red',alpha=0.5)
plt.ylabel('No. of missing values')
plt.title('Missing values')

#Plot frequency of numerical columns 
plt.figure(figsize=(4,6))

fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)
df['Price'].plot(ax=ax1, kind='hist', bins=np.arange(30))
df['Quantity'].plot(ax=ax2, kind='hist',bins=np.arange(100))

ax1.title.set_text('Price')
ax2.title.set_text('Quantity')

#To get number of unique values of each column of the
df.nunique()

#Plot of unique values

plt.figure()
fig, ax = plt.subplots()
df.nunique().plot(ax=ax, kind='bar')

"""## Data Cleaning"""

count= df["Invoice"].count()
count

#Data analysis
df.StockCode.value_counts()

df.StockCode.unique()

#Returns the count of the values of each Customer ID

df["Customer ID"].value_counts(dropna=True)

len(df["Customer ID"])

#returns the rowns with any null values present

df[df.isnull().any(axis=1)]

#Drop the null values from the dataset and create dataset df_new

df_new= df.dropna()

df_new.head()

#Dataframe for a specific stock code checked
df[(df.Invoice=='C489559')]

#check again for null values count
df_new.isnull().sum().sort_values(ascending=False)

#Cancelled invoices logic--

#string to be searched at the beginning of column data  
search ="C"
  
df_new[df_new["Invoice"].astype(str).str.startswith(search)]

df_new[df_new["Invoice"]=='C489449']

#all the cancelled invoices have negative quantities

#to check if any negative price exists for the invoices that are not cancelled
df_new[(df_new["Invoice"].astype(str).str.startswith(search)) & (df_new['Price']<0)]

#to check if any negative price exists for the invoices that are not cancelled
df_new[(~df_new["Invoice"].astype(str).str.startswith(search)) & (df_new['Price']<0)]

#to check if any negative quantity exists for the invoices that are not cancelled
df_new[(~df_new["Invoice"].astype(str).str.startswith(search)) & (df_new['Quantity']<0)]

#All the cancelled have negative quantities and therefore we can separate them into another dataframe
df_new = df_new[(df_new.Quantity > 0)]
df_cancelled= df_new[(df_new.Quantity < 0)]

#all the operations will be performed on our clean data taken in the dataframe df_new
df_new.count()

df_new.describe().round(2)

#Change column type of Customer ID to Int
df_new['Customer ID'] = df_new['Customer ID'].astype('int64')

# change description to LOWER case
df_new['Description'] = df_new.Description.str.lower()

df_new.head()

#df_new.groupby("Country").Price.mean().sort_values(ascending=False)[:5].plot.bar()

#Plot numerical columns
plt.figure()

fig, (ax1, ax2) = plt.subplots(1, 2)
df_new['Price'].plot(ax=ax1, kind='hist',bins=np.arange(30))
df_new['Quantity'].plot(ax=ax2, kind='hist',bins=np.arange(90))

ax1.title.set_text('Price')
ax2.title.set_text('Quantity')

df_new.describe()

#To get the year from the column Invoice Year

df_new['InvoiceYear']= df_new['InvoiceDate'].dt.year

#To get the month from the column Invoice Month

df_new['InvoiceMonth']= df_new['InvoiceDate'].dt.month

df_new.head()

#Amount is defined as the product of quantity and unit price of the stock product
df_new['Amount'] = (df_new['Quantity'] * df_new['Price'])

#Weighted Price of all the prices 
df_new['WeightedAmount']= sum(df_new['Amount'])/sum(df_new['Quantity'])
df_new.head()

"""##Create a StockCode, Invoice Year, Invoice Month level data set and create Summarized columns"""

#grouped dataframe containing stockcode, invoice year and invoice month level data set

grouped=df_new.groupby(['StockCode','InvoiceYear','InvoiceMonth']).aggregate({'Invoice':'nunique','Quantity':['sum','max'], 'Customer ID':'nunique', 'Price':['nunique',lambda x:(x.nunique()-1)],'Amount':'sum'})
grouped.columns = ['TotalOrders','TotalQuantity','MaxQuantity','UniqueCustomerCount', 'UniquePriceCount','PriceChangeCount','TotalAmount']
grouped= grouped.reset_index()

grouped['WeightedPrice']= grouped['TotalAmount']/grouped['TotalQuantity']
grouped['PriceChangeCount']= grouped['PriceChangeCount'].astype(int)
grouped['BelowWeightedPrice']= grouped['WeightedPrice']- grouped['TotalAmount']
grouped['AboveWeightedPrice']= grouped['TotalAmount']-grouped['WeightedPrice']
grouped

MaxQuantity=df_new.groupby(by=['StockCode', 'InvoiceYear', 'InvoiceMonth'], as_index=False).aggregate({'Quantity' :'max','Customer ID': 'min'})
MaxQuantity2=df_new.groupby(by=['StockCode', 'InvoiceYear', 'InvoiceMonth', 'Quantity'], as_index=False).aggregate({'Customer ID': 'min'})

#print(MaxQuantity)
#print(MaxQuantity2)

df_joined = pd.merge(grouped, MaxQuantity, left_on=['StockCode', 'InvoiceYear', 'InvoiceMonth', 'MaxQuantity'], right_on=['StockCode', 'InvoiceYear', 'InvoiceMonth', 'Quantity'], how='left', sort=True)
df_3 = df_joined[['StockCode', 'InvoiceYear', 'InvoiceMonth', 'UniqueCustomerCount', 'UniquePriceCount', 'Customer ID', 'PriceChangeCount', 'TotalOrders', 'WeightedPrice', 'TotalAmount', 'BelowWeightedPrice', 'AboveWeightedPrice', 'TotalQuantity', 'MaxQuantity']]
df_3

#Generating csv file as output

csv_data = grouped.to_csv(r'/content/assignmentA3.csv', index = False, header=True, sep='|')

"""##Insights

Insight- 1

*   The total number of orders/invoices of each month compared for the different years in the dataset- 2009,2010,2011
*   The maximum number of orders or invoicing happened in the month of November for both the years 2010 and 2011.
"""

pv = pd.pivot_table(grouped, index=grouped.InvoiceMonth, columns=grouped.InvoiceYear,
                    values='TotalOrders', aggfunc='sum')
ax = pv.plot(lw=2, colormap='jet', marker='.', markersize=10, title='Monthly Invoice - Total no of orders in 2009, 2010 and 2011')
ax.set_ylabel("Total no of orders")

"""Insight- 2


*   The total amount of products sold in the given years of the dataset
*   The year 2011 saw a slight fall in the total amount as compared to the sales in 2010
"""

yearly_amount = grouped.groupby(by=['InvoiceYear'], as_index=False)['TotalAmount'].sum()
pt = yearly_amount.plot(kind='bar', x='InvoiceYear', y='TotalAmount')
pt.set_ylabel("Total Amount")

"""Insight- 3



*   Below I have spooled out a list of top 5 StockCodes, with the highest UniqueCustomerCount.
*   These can be considered as popular stocks among varied customers
"""

frame = grouped.sort_values(by='UniqueCustomerCount', ascending=False).groupby('StockCode')['UniqueCustomerCount'].sum()
frame2 = pd.DataFrame(frame.reset_index(name = "UniqueCustomerCount"))
frame_sorted = frame2.sort_values('UniqueCustomerCount', ascending=False).head(5)

sns.barplot(x='StockCode', y='UniqueCustomerCount', data=frame_sorted)

"""Insight - 4


*   This visualization is a bird's eye view on the number of instances when Price changes in a given month-year.
*   As we can see, for the most part, price did not change (count was 0). To further magnify the instances when it did change, we need to group by the StockCode and analyze which commodities faced how many price change instances.
"""

sns.countplot(x='PriceChangeCount', data=grouped)

"""## Grouping based on Description

Spliting the description column from the main dataframe and saving it as  list in "flat_list".
"""

trial = df_new.Description.str.split().to_list()
trial

flat_list = []
trial
for sublist in trial :
    for item in sublist :
        flat_list.append(item)
len(flat_list)

"""Using 'nltk' package, I removed the common english stock words which would have greater word count compared to others. From here I get the filtered_list."""

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords 
from nltk.tokenize import word_tokenize
stop_words = set(stopwords.words('english')) 
filtered_list=[]
for w in flat_list:
    if w not in stop_words: 
        filtered_list.append(w)
filtered_list

"""In the filtered_list, I tag the words using the'nlkt' tags. Main idea behind the same is to remove the advejectives, verbs or connectors of the English language and keep only the nouns."""

#import re 
nltk.download('averaged_perceptron_tagger')
tagged_words = nltk.pos_tag(filtered_list)

tagged_wordframe = pd.DataFrame(tagged_words, columns=['Word','Tag'])
tagged_wordframe2 = tagged_wordframe[tagged_wordframe['Tag']!='CD']

#Removing the Unwanted Tags like - $ | , | : and only considering the words tagged as Nouns i.e. 'NN' and 'NNS'
tagged_wordframe_nouns = tagged_wordframe2[(tagged_wordframe2['Tag']=='NN') | (tagged_wordframe2['Tag']=='NNS')]
print(tagged_wordframe_nouns)

#Counting the occurences for different words in the filtered list

count_list = tagged_wordframe_nouns['Word'].value_counts().rename_axis('Word').reset_index(name='Counts')
count_list

"""So now I have a count of nouns in the column.

Loop through each cell of the description column, clean the column, match the cleaned keywords with the keywords in the count_list and set the keyword with max count as the Group Name
"""

unique = df_new['Description'].unique()
df_unique = pd.DataFrame(unique, columns=['Description'])
trial_unique = df_unique.Description.str.split().to_list()
x = []
counter = 0
for item in trial_unique:
    print(item)
    filtered_list = [x for x in item if x not in stop_words]
    tagged_list = nltk.pos_tag(filtered_list)
    tagged_words = pd.DataFrame(tagged_list, columns=['Word','Tag'])
    #Keeping only NN and NNS
    tagged_wordframe = tagged_words[(tagged_words['Tag']=='NN') | (tagged_words['Tag']=='NNS')]
    print(tagged_wordframe)
    if(len(tagged_wordframe) != 0):
        df_joined = pd.merge(tagged_wordframe, count_list, on="Word", how="left")
        df_joined2 = df_joined.dropna()
        if(len(df_joined2) != 0):
            y = df_joined2['Word'].loc[df_joined2['Counts'].idxmax()]
            counter = counter + 1
        else:
            y = "Unknown"
            counter = counter + 1
    else:
        y = "Unknown"
        counter = counter + 1
    print(y)
    print("Iteration: ", counter)
    x.append(y)
    #df_new[df_new.Description.str.split().tolist() == item]['Word'] = df_joined['Word'].loc[df_joined['Counts'].idxmax()]
df_unique['Word']=x

df_combined = pd.merge(df_new, df_unique, on='Description', how='inner')
df_combined.rename(columns={'Word': 'Group'}, inplace=True)
df_combined

"""Now to group the dataset with respect to the Group Word identified:"""

df_final = df_combined[['Group', 'Description', 'StockCode']].drop_duplicates()
df_final.reset_index(drop=True, inplace=True)
df_final= df_final[df_final['Group'] != 'Unknown']
df_final.sort_values(by='Group', inplace=True, ascending=True)
df_final

#Generating csv file as output
import csv
df_final.to_csv(r'/content/assignment6.csv', index = False, header=True, sep='|', quoting=csv.QUOTE_NONE, quotechar="", escapechar="\\")